{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quadro M1000M'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import math\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from utilities import *\n",
    "from IPython import get_ipython\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "np.random.seed(1)\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "#torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "cuda0 = torch.device('cuda:0')\n",
    "dtype = torch.cuda.FloatTensor\n",
    "from torch.autograd import Variable\n",
    "torch.cuda.get_device_name(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "\n",
    "ipython = get_ipython()\n",
    "if '__IPYTHON__' in globals():\n",
    "    ipython.magic('load_ext autoreload')\n",
    "    ipython.magic('autoreload 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModule():\n",
    "\n",
    "\n",
    "    def Conv2D(self,features=None,n_filters=None,kernal_size=None,stride=None,padding=None,layer_name=None):\n",
    "        \n",
    "        self.features = features\n",
    "        self.n_filters = n_filters\n",
    "        self.kernal_size = kernal_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.layer_name = layer_name\n",
    "        # Retrieve dimensions from A_prev's shape  \n",
    "        (self.mini_batch_size, self.img_height_prev, self.img_Width_prev, self.n_channels_prev) = self.features.shape\n",
    "        self.weight,self.bias = self.weight_bias_initializer(self.layer_name)\n",
    "        # Retrieve dimensions from W's shape \n",
    "        (self.filter_size, self.filter_size, self.n_channels_prev, self.n_channels_new) = self.weight.shape\n",
    "    \n",
    "\n",
    "    \n",
    "        # Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor. (≈2 lines)\n",
    "        self.img_height_new = int(((self.img_height_prev - self.filter_size + 2*self.padding)/self.stride) + 1)\n",
    "        self.img_width_new = int(((self.img_Width_prev - self.filter_size + 2*self.padding)/self.stride) + 1)\n",
    "    \n",
    "        # Initialize the output volume Z with zeros.\n",
    "        self.Z = np.zeros((self.mini_batch_size,self.img_height_new,self.img_width_new,self.n_channels_new))\n",
    "    \n",
    "        # Create A_prev_pad by padding A_prev\n",
    "        self.feature_padding = np.pad(self.features,((0,0),(self.padding,self.padding),(self.padding,self.padding),(0,0)),'constant')\n",
    "    \n",
    "\n",
    "        # Select ith training example's padded activation\n",
    "        for h in range(self.img_height_new):                           # loop over vertical axis of the output volume\n",
    "            for w in range(self.img_width_new):                       # loop over horizontal axis of the output volume\n",
    "                vert_start = h*self.stride \n",
    "                vert_end = self.filter_size+vert_start\n",
    "                horiz_start = w*self.stride \n",
    "                horiz_end = self.filter_size + horiz_start\n",
    "                    \n",
    "                # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell).\n",
    "                a_slice_prev =self.feature_padding[:,vert_start :vert_end,horiz_start:horiz_end,:]\n",
    "                a_slice_prev = a_slice_prev.reshape(a_slice_prev.shape[0], -1).T\n",
    "                WW=np.concatenate(np.concatenate(self.weight[:,:,:,:])).T\n",
    "                SS=np.dot(WW,a_slice_prev).reshape(self.n_channels_new,self.mini_batch_size)\n",
    "                self.Z[:, h, w, :]=(SS+self.bias[:,:,:,:].reshape(self.n_channels_new,1)).T\n",
    "\n",
    "\n",
    "\n",
    "        # Making sure your output shape is correct\n",
    "        assert(self.Z.shape == (self.mini_batch_size,self.img_height_new,self.img_width_new,self.n_channels_new))\n",
    "    \n",
    "        # Save information in \"cache\" for the backprop\n",
    "        cache = (self.features,self.weight, self.bias,self.stride,self.padding)\n",
    "    \n",
    "        return self.Z, cache\n",
    "        \n",
    "\n",
    "\n",
    "    def MaxPool(self,conv_layer_output,pool_layer_stride,pool_layer_filter_size,mode=\"max\"):\n",
    "        \n",
    "        self.conv_layer_output = conv_layer_output\n",
    "        self.pool_layer_filter_size = pool_layer_filter_size\n",
    "        self.pool_layer_stride = pool_layer_stride\n",
    "        \n",
    "        self.mode = mode\n",
    "    \n",
    "        # Retrieve dimensions from the input shape\n",
    "        (self.mini_batch_size, self.img_height_prev, self.img_Width_prev, self.n_channels_prev) = self.conv_layer_output.shape\n",
    "\n",
    "        # Retrieve hyperparameters from \"hparameters\"\n",
    "\n",
    "        # Define the dimensions of the output\n",
    "        self.img_height_new = int(1 + (self.img_height_prev - self.pool_layer_filter_size) / self.pool_layer_stride)\n",
    "        self.img_width_new = int(1 + (self.img_Width_prev - self.pool_layer_filter_size) / self.pool_layer_stride)\n",
    "        self.n_channels_new = self.n_channels_prev\n",
    "    \n",
    "        # Initialize output matrix A\n",
    "        max_pooling_output = np.zeros((self.mini_batch_size, self.img_height_new, self.img_width_new,self.n_channels_new)) \n",
    "    \n",
    "        for h in range(self.img_height_new):                     # loop on the vertical axis of the output volume\n",
    "            for w in range(self.img_width_new):                  # loop on the horizontal axis of the output volume\n",
    "                for c in range (self.n_channels_new):            # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h*self.pool_layer_stride\n",
    "                    vert_end = self.pool_layer_filter_size+vert_start\n",
    "                    horiz_start = w*self.pool_layer_stride\n",
    "                    horiz_end = self.pool_layer_filter_size+ horiz_start\n",
    "                    \n",
    "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c.\n",
    "                    a_prev_slice = self.conv_layer_output[:,vert_start :vert_end,horiz_start:horiz_end,c]\n",
    "                    a_prev_slice = a_prev_slice.reshape(a_prev_slice.shape[0], -1).T\n",
    "                    \n",
    "                    \n",
    "                    # Compute the pooling operation on the slice. Use an if statment to differentiate the modes. Use np.max/np.mean.\n",
    "                    if self.mode == \"max\":\n",
    "                        max_pooling_output[:, h, w, c] = np.max(a_prev_slice,axis=0)\n",
    "                    elif self.mode == \"average\":\n",
    "                        max_pooling_output[:, h, w, c] = np.mean(a_prev_slice,axis=0)\n",
    "    \n",
    "    \n",
    "        # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "        cache = (self.conv_layer_output, self.pool_layer_stride,self.pool_layer_filter_size)\n",
    "    \n",
    "        # Making sure your output shape is correct\n",
    "        assert(max_pooling_output.shape == (self.mini_batch_size, self.img_height_new, self.img_width_new,self.n_channels_new))\n",
    "    \n",
    "        return max_pooling_output, cache\n",
    "    \n",
    "    def Dropout(self,activation_output,keep_prop):\n",
    "        drop_array = np.random.rand(*activation_output.shape) < keep_prop\n",
    "        dropout_output = np.multiply(activation_output,drop_array)\n",
    "        dropout_output /=keep_prop\n",
    "        return dropout_output\n",
    "    \n",
    "    def Flatten(self):\n",
    "        pass\n",
    "    \n",
    "    def Activation(self,inputs,activation_type=\"relu\"):\n",
    "        if activation_type==\"softmax\":\n",
    "            input_shift = inputs - np.max(inputs)\n",
    "            activation_output= np.exp(input_shift) / np.sum(np.exp(input_shift), axis=0,keepdims=True)\n",
    "            return activation_output,inputs\n",
    "        \n",
    "        else:\n",
    "            # relu activation function\n",
    "            activation_output = np.maximum(inputs,0)\n",
    "        \n",
    "        return activation_output,inputs\n",
    "        \n",
    "\n",
    "    \n",
    "    def weight_bias_initializer(self,layer_name):\n",
    "        parameters={}\n",
    "        np.random.seed(1)\n",
    "        ###### initialize the weights :\n",
    "    \n",
    "        parameters[\"W\"+self.layer_name] = np.random.randn(self.kernal_size,self.kernal_size,\n",
    "                                                          self.n_channels_prev,self.n_filters)*np.sqrt(2/(self.n_channels_prev*(self.kernal_size)**2))\n",
    "        \n",
    "        parameters[\"b\" + self.layer_name] = np.zeros((1,1,1,self.n_filters))\n",
    "        return parameters[\"W\"+self.layer_name],parameters[\"b\" + self.layer_name]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    # train dataset\n",
    "    train_dataset = h5py.File(r'train_signs.h5', \"r\")\n",
    "    train_set_x_orig = torch.tensor(train_dataset[\"train_set_x\"][:]).to(cuda0) # train set features\n",
    "    train_set_y_orig = torch.LongTensor(train_dataset[\"train_set_y\"][:]).to(cuda0) # train set labels\n",
    "    \n",
    "    # test dataset\n",
    "    test_dataset = h5py.File(r'test_signs.h5', \"r\")\n",
    "    test_set_x_orig = torch.tensor(test_dataset[\"test_set_x\"][:]).to(cuda0)# test set features\n",
    "    test_set_y_orig = torch.LongTensor(test_dataset[\"test_set_y\"][:]).to(cuda0) # test set labels\n",
    "    \n",
    "    # classes in dataset\n",
    "    classes = torch.LongTensor(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    # reshape the labels in test and train dataset\n",
    "    train_set_y_orig = train_set_y_orig.view(train_set_y_orig.shape[0],1)\n",
    "    test_set_y_orig = test_set_y_orig.view(test_set_y_orig.shape[0],1)\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset in more details\n",
    "\n",
    "##### train_set_x_orig shape = (1080, 64, 64, 3)\n",
    "##### train_set_y_orig shape = (1080, 1)\n",
    "##### test_set_x_orig shape  = (120, 64, 64, 3)\n",
    "##### test_set_y_orig shape  = (120, 1)\n",
    "##### Classes =([0, 1, 2, 3, 4, 5], dtype=int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the lables into one-hot-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = torch.t(torch.eye(C)[Y.view(-1)])\n",
    "    return Y.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_yoh = convert_to_one_hot(train_y,len(classes))# the new shape is (1080,6) \n",
    "test_yoh  = convert_to_one_hot(test_y,len(classes))  # the new shape is (120,6)\n",
    "train_yoh.type()\n",
    "train_yoh.get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig=train_x_orig/255   # shape (1080, 64, 64, 3)\n",
    "test_x_orig=test_x_orig/255    # shape (120, 64, 64, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples) (m, Hi, Wi, Ci)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) (m, n_y)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(torch.randperm(m))\n",
    "    shuffled_X = X[permutation,:,:,:]\n",
    "    shuffled_Y = Y[:,permutation]\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n",
    "        mini_batch_Y = shuffled_Y[:,k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:,:]\n",
    "        mini_batch_Y = shuffled_Y[:,num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Filters for Convolutional Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_he():\n",
    "    \n",
    "    parameters={}\n",
    "    torch.manual_seed(1)\n",
    "    ###### initialize the weights :\n",
    "    \n",
    "    parameters[\"W1\"] = torch.randn(5,5,3,32).cuda().type(dtype)*np.sqrt(2/((3)*25))\n",
    "    parameters[\"W2\"] = torch.randn(3,3,32,64).cuda().type(dtype)*np.sqrt(2/((32)*9)) \n",
    "    parameters[\"W3\"] = torch.randn(3,3,64,64).cuda().type(dtype)*np.sqrt(2/((64)*9))\n",
    "    parameters[\"W4\"] = torch.randn(3,3,64,64).cuda().type(dtype)*np.sqrt(2/((64)*9))\n",
    "    \n",
    "    # fully connected layer with 256 neurons\n",
    "    parameters[\"W5\"] = torch.randn(256,1024).cuda().type(dtype)*np.sqrt(2/((1024)))\n",
    "    # output layer with 6 neurons (the number of classes)\n",
    "    parameters[\"W6\"] = torch.randn(6,256).cuda().type(dtype)*np.sqrt(2/((256)))\n",
    "    \n",
    "    ###### initialize the bais\n",
    "    parameters[\"b1\"] = torch.zeros((1,1,1,32)).cuda().type(dtype)\n",
    "    parameters[\"b2\"] = torch.zeros((1,1,1,64)).cuda().type(dtype)\n",
    "    parameters[\"b3\"] = torch.zeros((1,1,1,64)).cuda().type(dtype)\n",
    "    parameters[\"b4\"] = torch.zeros((1,1,1,64)).cuda().type(dtype)\n",
    "    parameters[\"b5\"] = torch.zeros((256,1)).cuda().type(dtype)\n",
    "    parameters[\"b6\"] = torch.zeros((6,1)).cuda().type(dtype)\n",
    " \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the  Adam Parameters ( We need that to update the weight and bias with Adam optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters) :\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
    "    for l in range(L):\n",
    "        if(l>3):\n",
    "            v[\"dW\" + str(l+1)] = torch.zeros(( parameters[\"W\" + str(l+1)].shape[0],parameters[\"W\" + str(l+1)].shape[1])).cuda().type(dtype)\n",
    "            v[\"db\" + str(l+1)] = torch.zeros(( parameters[\"b\" + str(l+1)].shape[0],parameters[\"b\" + str(l+1)].shape[1])).cuda().type(dtype)\n",
    "            s[\"dW\" + str(l+1)] = torch.zeros(( parameters[\"W\" + str(l+1)].shape[0],parameters[\"W\" + str(l+1)].shape[1])).cuda().type(dtype)\n",
    "            s[\"db\" + str(l+1)] = torch.zeros(( parameters[\"b\" + str(l+1)].shape[0],parameters[\"b\" + str(l+1)].shape[1])).cuda().type(dtype)\n",
    "\n",
    "        else:   \n",
    "            v[\"dW\" + str(l+1)] = torch.zeros(( parameters[\"W\" + str(l+1)].shape[0],parameters[\"W\" + str(l+1)].shape[1],parameters[\"W\" + str(l+1)].shape[2],parameters[\"W\" + str(l+1)].shape[3])).cuda().type(dtype)\n",
    "            v[\"db\" + str(l+1)] = torch.zeros(( parameters[\"b\" + str(l+1)].shape[0],parameters[\"b\" + str(l+1)].shape[1],parameters[\"b\" + str(l+1)].shape[2],parameters[\"b\" + str(l+1)].shape[3])).cuda().type(dtype)\n",
    "            s[\"dW\" + str(l+1)] = torch.zeros(( parameters[\"W\" + str(l+1)].shape[0],parameters[\"W\" + str(l+1)].shape[1],parameters[\"W\" + str(l+1)].shape[2],parameters[\"W\" + str(l+1)].shape[3])).cuda().type(dtype)\n",
    "            s[\"db\" + str(l+1)] = torch.zeros(( parameters[\"b\" + str(l+1)].shape[0],parameters[\"b\" + str(l+1)].shape[1],parameters[\"b\" + str(l+1)].shape[2],parameters[\"b\" + str(l+1)].shape[3])).cuda().type(dtype)\n",
    "\n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions to apply convolution in an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n",
    "    as illustrated in Figure 1.\n",
    "\n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "\n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\"\n",
    "    X = X.transpose(1,3).transpose(2,3)\n",
    "    m = nn.ZeroPad2d((pad, pad, pad, pad))\n",
    "    X = m(X)\n",
    "    X_pad = X.transpose(1,3).transpose(1,2)\n",
    "    #X_pad = torch.from_numpy(np.pad(X, ((0,0), (pad,pad), (pad,pad), (0,0)), 'constant', constant_values = (0,0)))\n",
    "    \n",
    "\n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
    "    of the previous layer.\n",
    "\n",
    "    Arguments:\n",
    "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\"\n",
    "\n",
    "    # Element-wise product between a_slice and W. Do not add the bias yet.\n",
    "    s = torch.mul(a_slice_prev,W)\n",
    "    # Sum over all entries of the volume s.\n",
    "    Z = torch.sum(s)\n",
    "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
    "    Z = Z+b\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape  \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape \n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\" \n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    # Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor. (≈2 lines)\n",
    "    n_H = int(((n_H_prev - f + 2*pad)/stride) + 1)\n",
    "    n_W = int(((n_W_prev - f + 2*pad)/stride) + 1)\n",
    "    \n",
    "    # Initialize the output volume Z with zeros.\n",
    "    Z = torch.zeros((m,n_H,n_W,n_C)).cuda().type(dtype)\n",
    "    \n",
    "    # Create A_prev_pad by padding A_prev\n",
    "    \n",
    "    A_prev_pad = zero_pad(A_prev,pad)\n",
    "    \n",
    "\n",
    "        # Select ith training example's padded activation\n",
    "    for h in range(n_H):                           # loop over vertical axis of the output volume\n",
    "        for w in range(n_W):   \n",
    "     \n",
    "            # loop over horizontal axis of the output volume\n",
    "            vert_start = h*stride #stride=2\n",
    "            vert_end = f+vert_start\n",
    "            horiz_start = w*stride # stride=2\n",
    "            horiz_end = f + horiz_start\n",
    "            # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell).\n",
    "            a_slice_prev =A_prev_pad[:,vert_start :vert_end,horiz_start:horiz_end,:]\n",
    "            a_slice_prev = a_slice_prev.contiguous()\n",
    "            a_slice_prev = a_slice_prev.view(-1,a_slice_prev.shape[0])\n",
    "            WW= W.view(W.shape[-1],-1)\n",
    "            SS=torch.mm(WW,a_slice_prev.type(dtype)).view(n_C,m)\n",
    "            Z[:, h, w, :]=torch.t(SS+b.view(n_C,1))\n",
    "\n",
    "\n",
    "\n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pooling Layer Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "\n",
    "    \n",
    "    # Retrieve dimensions from the input shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    # Define the dimensions of the output\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    # Initialize output matrix A\n",
    "    A = torch.zeros((m, n_H, n_W, n_C)).cuda().type(dtype)\n",
    "    \n",
    "    for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "        for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "            for c in range (n_C):            # loop over the channels of the output volume\n",
    "                    \n",
    "                # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                vert_start = h*stride\n",
    "                vert_end = f+vert_start\n",
    "                horiz_start = w*stride\n",
    "                horiz_end = f + horiz_start\n",
    "                    \n",
    "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c.\n",
    "                a_prev_slice = A_prev[:,vert_start :vert_end,horiz_start:horiz_end,c]\n",
    "                a_prev_slice = a_prev_slice.contiguous()\n",
    "                a_prev_slice = torch.t(a_prev_slice.view(a_prev_slice.shape[0], -1))\n",
    "                #print(torch.max(a_prev_slice,dim=0),np.max(a_prev_slice.cpu().numpy(),axis=0))\n",
    "                    # Compute the pooling operation on the slice. Use an if statment to differentiate the modes. Use np.max/np.mean.\n",
    "                if mode == \"max\":\n",
    "                    A[:, h, w, c] = torch.max(a_prev_slice,dim=0)[0]\n",
    "                elif mode == \"average\":\n",
    "                    A[:, h, w, c] = torch.mean(a_prev_slice,dim=0)\n",
    "    \n",
    "    \n",
    "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layer Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "\n",
    "\n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = torch.zeros((m, n_H_prev, n_W_prev, n_C_prev)).cuda().type(dtype)                        \n",
    "    dW = torch.zeros((f, f, n_C_prev, n_C)).cuda().type(dtype)\n",
    "    db = torch.zeros((1,1,1,n_C)).cuda().type(dtype)\n",
    "\n",
    "    # Pad A_prev and dA_prev\n",
    "    A_prev_pad =zero_pad(A_prev,pad)\n",
    "    dA_prev_pad = zero_pad(A_prev,pad)   \n",
    "    \n",
    "    # loop over the training examples\n",
    "        \n",
    "\n",
    "        \n",
    "    for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "        for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "\n",
    "            vert_start = h*stride\n",
    "            vert_end = f+vert_start\n",
    "            horiz_start = w*stride\n",
    "            horiz_end = f+ horiz_start\n",
    "                    \n",
    "            # Use the corners to define the slice from a_prev_pad\n",
    "            a_slice =A_prev_pad[:,vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "  \n",
    "\n",
    "            # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "            dA_prev_pad[:,vert_start:vert_end, horiz_start:horiz_end, :] += torch.sum(W*torch.sum(dZ[:, h, w, :],dim=0).view(1,1,1,n_C),dim=-1)/m\n",
    "            dW += torch.sum(a_slice.type(dtype).view(a_slice.shape[0],a_slice.shape[1],a_slice.shape[2],a_slice.shape[3],1)*dZ[:, h, w, :].view(m,1,1,1,n_C),dim=0)\n",
    "            #print(a_slice*dZ[:, h, w, c].reshape(m,1,1,1))\n",
    "\n",
    "\n",
    "                        \n",
    "            db += torch.sum(torch.sum(dZ[:, h, w, :],dim=0).view(1,1,1,n_C),dim=-1)/n_C\n",
    "            \n",
    "              \n",
    "                \n",
    "    # Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
    "    dA_prev[:, :, :, :] = dA_prev_pad[:,pad:-pad,pad:-pad,:]\n",
    "\n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pooling Layer Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x,shape):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Array of shape (f, f)\n",
    "    \n",
    "    Returns:\n",
    "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
    "    \"\"\"\n",
    "    x=torch.t(x.contiguous().view(x.shape[0],-1))\n",
    "\n",
    "    mask = (x==torch.max(x,dim=0)[0]).type(dtype)\n",
    "    mask = mask.view(shape)\n",
    "\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Retrieve information from cache \n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\" \n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape \n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    # Initialize dA_prev with zeros \n",
    "    dA_prev = torch.zeros(A_prev.shape).cuda().type(dtype)\n",
    "    #a_slice_shape=[]\n",
    "        # select training example from A_prev \n",
    "        \n",
    "    for h in range(n_H):                   # loop on the vertical axis\n",
    "        for w in range(n_W):               # loop on the horizontal axis\n",
    "            for c in range(n_C):           # loop over the channels (depth)\n",
    "                    \n",
    "                # Find the corners of the current \"slice\" \n",
    "                vert_start = h*stride\n",
    "                vert_end = f+ vert_start\n",
    "                horiz_start = w*stride\n",
    "                horiz_end = f+horiz_start\n",
    "                    \n",
    "                # Compute the backward propagation in both modes.\n",
    "                if mode == \"max\":\n",
    "                        \n",
    "                    # Use the corners and \"c\" to define the current slice from a_prev \n",
    "                    a_prev_slice = A_prev[:,vert_start:vert_end,horiz_start:horiz_end,c]\n",
    "                    a_slice_shape=a_prev_slice.shape\n",
    "\n",
    "                    # Create the mask from a_prev_slice (≈1 line)\n",
    "                    mask = create_mask_from_window(a_prev_slice,a_slice_shape)\n",
    "\n",
    "                    dA_prev[:, vert_start: vert_end, horiz_start: horiz_end, c] += torch.mul(mask,dA[:,h,w,c].view(m,1,1))\n",
    "\n",
    "                        \n",
    "  \n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "\n",
    "    \n",
    "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
    "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
    "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
    "    \n",
    "    print(type(v[\"dW1\"]),type(s[\"dW1\"]),type(parameters[\"W1\"]))\n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(L):\n",
    "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
    "      \n",
    "        v[\"dW\" + str(l+1)] = beta1*v[\"dW\" + str(l+1)]+ (1-beta1)*grads[\"dW\" + str(l+1)]\n",
    "        v[\"db\" + str(l+1)] = beta1*v[\"db\" + str(l+1)]+ (1-beta1)*grads[\"db\" + str(l+1)]\n",
    "\n",
    "\n",
    "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
    "     \n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)]/(1-(beta1**t))\n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)]/(1-(beta1**t))\n",
    "        \n",
    "\n",
    "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
    "        \n",
    "        s[\"dW\" + str(l+1)] = beta2*s[\"dW\" + str(l+1)]+ (1-beta2)*(grads[\"dW\" + str(l+1)]**2)\n",
    "        s[\"db\" + str(l+1)] = beta2*s[\"db\" + str(l+1)]+ (1-beta2)*(grads[\"db\" + str(l+1)]**2)\n",
    "       \n",
    "\n",
    "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
    "\n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)]/(1-(beta2**t))\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)]/(1-(beta2**t))\n",
    "    \n",
    "\n",
    "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*(v_corrected[\"dW\" + str(l+1)]/(torch.sqrt(s_corrected[\"dW\" + str(l+1)]) + epsilon))\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*(v_corrected[\"db\" + str(l+1)]/(torch.sqrt(s_corrected[\"db\" + str(l+1)]) + epsilon))\n",
    "   \n",
    "\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters = initialize_parameters_he()\n",
    "#mini_batches=random_mini_batches(train_x_orig, train_y, mini_batch_size=64, seed=0)\n",
    "#forward_prpagation(mini_batches[0][0],mini_batches[0][1],parameters)\n",
    "\n",
    "def Model_Backward(AL,Y,caches,P4_shape,parameters,lambd):\n",
    "    m=Y.shape[0]\n",
    "    grads={}\n",
    "    dAL=-torch.div(Y,AL)     \n",
    "    \n",
    "    linear_cache5,Z6=caches[5]\n",
    "    A5,W6,b6=linear_cache5\n",
    "    \n",
    "    dZ6=softmax_backward(Z6,Y) \n",
    "    \n",
    "    grads[\"dW6\"]=(1./m)*torch.mm(dZ6,torch.t(A5)) + (lambd/m)*parameters[\"W6\"]\n",
    "    grads[\"db6\"] = (1./m) * torch.sum(dZ6, dim = 1, keepdim = True)\n",
    "    #print(\"grads db6 :\" , grads[\"db6\"])\n",
    "    #print(\"grads dW6 :\" , grads[\"dW6\"])\n",
    "    \n",
    "    dA5 = torch.t(torch.mm(torch.t(W6),dZ6))\n",
    "    \n",
    "    \n",
    "    linear_cache4,Z5=caches[4]\n",
    "    P4,W5,b5=linear_cache4\n",
    "    dZ5=relu_backward(dA5,torch.t(Z5))\n",
    "    grads[\"dW5\"]=(1./m)*torch.mm(torch.t(dZ5),torch.t(P4)) + (lambd/m)*parameters[\"W5\"]\n",
    "    grads[\"db5\"] = (1./m) * torch.sum(torch.t(dZ5), dim = 1, keepdim = True)\n",
    "    #print(\"grads db5 :\" , grads[\"db5\"])\n",
    "    #print(\"grads dW5 :\" , grads[\"dW5\"])\n",
    "    linear_cache3,Z4,pool_cache3=caches[3]\n",
    "    \n",
    "    dP4=torch.t(torch.mm(dZ5,W5))\n",
    "    dP4=dP4.contiguous().view(P4_shape)\n",
    "    dA4=pool_backward(dP4, pool_cache3, mode = \"max\")\n",
    "\n",
    "    linear_cache2,Z3,pool_cache2=caches[2]\n",
    "    \n",
    "    dZ4=relu_backward(dA4, Z4)\n",
    "    dP3,dW4,db4=conv_backward(dZ4, linear_cache3)\n",
    "    grads[\"dW4\"]=dW4 + (lambd/m)*parameters[\"W4\"]\n",
    "    grads[\"db4\"]=db4\n",
    "    \n",
    "    dA3=pool_backward(dP3, pool_cache2, mode = \"max\")\n",
    "    dZ3=relu_backward(dA3, Z3)\n",
    "    dP2,dW3,db3=conv_backward(dZ3, linear_cache2)\n",
    "    grads[\"dW3\"]=dW3 + (lambd/m)*parameters[\"W3\"]\n",
    "    grads[\"db3\"]=db3\n",
    "    \n",
    "    linear_cache1,Z2,pool_cache1=caches[1]\n",
    "    \n",
    "    dA2=pool_backward(dP2, pool_cache1, mode = \"max\")\n",
    "    dZ2=relu_backward(dA2, Z2)\n",
    "    dP1,dW2,db2=conv_backward(dZ2, linear_cache1)\n",
    "    grads[\"dW2\"]=dW2 + (lambd/m)*parameters[\"W2\"]\n",
    "    grads[\"db2\"]=db2\n",
    "    \n",
    "    linear_cache0,Z1,pool_cache0=caches[0]\n",
    "    \n",
    "    dA1=pool_backward(dP1, pool_cache0, mode = \"max\")\n",
    "    dZ1=relu_backward(dA1, Z1)\n",
    "    dA,dW1,db1=conv_backward(dZ1, linear_cache0)\n",
    "    grads[\"dW1\"]=dW1 + (lambd/m)*parameters[\"W1\"]\n",
    "    grads[\"db1\"]=db1\n",
    "    \n",
    "    assert(b6.shape == grads[\"db6\"].shape)\n",
    "    assert(b5.shape == grads[\"db5\"].shape)\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def forward_prpagation(X,Y,parameters):\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    W3 = parameters['W3']\n",
    "    W4 = parameters['W4']\n",
    "    W5 = parameters['W5']\n",
    "    W6 = parameters['W6']\n",
    "\n",
    "    b1 = parameters['b1']\n",
    "    b2 = parameters['b2']\n",
    "    b3 = parameters['b3']\n",
    "    b4 = parameters['b4']\n",
    "    b5 = parameters['b5']\n",
    "    b6 = parameters['b6']\n",
    "    caches=[]\n",
    "    \n",
    "    #Convolution layer 1\n",
    "    model = CNNModule()\n",
    "    Z1, linear_cache = model.Conv2D(X, n_filters=32, kernal_size=5, stride=1,padding=2,layer_name=\"Conv_layer_1\")\n",
    "    A1, activation_cache = model.Activation(Z1,activation_type=\"relu\")\n",
    "    A1 = model.Dropout(A1,keep_prop=0.7)\n",
    "    P1,linear_pool_cache=model.MaxPool(A1,pool_layer_stride=2,pool_layer_filter_size=2, mode = \"max\")\n",
    "\n",
    "    \n",
    "    \n",
    "    caches.append((linear_cache,activation_cache,linear_pool_cache))\n",
    "    \n",
    "    \n",
    "    #Convolution layer 2\n",
    "    Z2,linear_cache=model.Conv2D(P1, n_filters=64, kernal_size=3, stride=1,padding=1,layer_name=\"Conv_layer_2\")\n",
    "    A2,activation_cache = model.Activation(Z2,activation_type=\"relu\")\n",
    "    A2 = model.Dropout(A2,keep_prop=0.60)\n",
    "    P2,linear_pool_cache=model.MaxPool(A2, pool_layer_stride=2,pool_layer_filter_size=2, mode = \"max\")\n",
    "    #print(P2.shape)\n",
    "    \n",
    "    caches.append((linear_cache,activation_cache,linear_pool_cache))\n",
    "    \n",
    "    \n",
    "    #Convolution layer 3\n",
    "    Z3,linear_cache=model.Conv2D(P2, n_filters=64, kernal_size=3, stride=1,padding=1,layer_name=\"Conv_layer_3\")\n",
    "    A3,activation_cache = model.Activation(Z3,activation_type=\"relu\")\n",
    "    A3 = model.Dropout(A3,keep_prop=0.8)\n",
    "    P3,linear_pool_cache=model.MaxPool(A3, pool_layer_stride=2,pool_layer_filter_size=2, mode = \"max\")\n",
    "\n",
    "    \n",
    "    caches.append((linear_cache,activation_cache,linear_pool_cache))\n",
    "    \n",
    "    #Convolution layer 4\n",
    "    Z4,linear_cache=model.Conv2D(P3, n_filters=64, kernal_size=3, stride=1,padding=1,layer_name=\"Conv_layer_4\")\n",
    "    A4,activation_cache = model.Activation(Z4,activation_type=\"relu\")\n",
    "    A4 = model.Dropout(A4,keep_prop=0.6)\n",
    "    P4,linear_pool_cache=model.MaxPool(A4, pool_layer_stride=2,pool_layer_filter_size=2, mode = \"max\")\n",
    "\n",
    "    \n",
    "    caches.append((linear_cache,activation_cache,linear_pool_cache))\n",
    "    \n",
    "    #flatten\n",
    "    P4_shape=P4.shape[:]\n",
    "\n",
    "    P4 = P4.reshape(P2.shape[0], -1).T\n",
    "\n",
    "    #Fully_Connected\n",
    "    \n",
    "    Z5,linear_cache= linear_forward(P4,W5,b5)\n",
    "    A5,activation_cache = model.Activation(Z5,activation_type=\"relu\")\n",
    "    A5 = model.Dropout(A5,keep_prop=0.8)\n",
    "    caches.append((linear_cache,activation_cache))\n",
    "    \n",
    "    Z6,linear_cache=linear_forward(A5,W6,b6)\n",
    "    A6,activation_cache = model.Activation(Z6,activation_type=\"softmax\")\n",
    "    caches.append((linear_cache,activation_cache))\n",
    "    cost,AL = cost_function(A6,Y,parameters,lambd = 0.1)\n",
    "    #print(sum(A3[0,:]))\n",
    "    return AL,caches,P4_shape,cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prpagation(X,Y,parameters):\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2'] \n",
    "    W3 = parameters['W3'] \n",
    "    W4 = parameters['W4'] \n",
    "    W5 = parameters['W5'] \n",
    "    W6 = parameters['W6']\n",
    "\n",
    "    b1 = parameters['b1']\n",
    "    b2 = parameters['b2']\n",
    "    b3 = parameters['b3']\n",
    "    b4 = parameters['b4']\n",
    "    b5 = parameters['b5']\n",
    "    b6 = parameters['b6']\n",
    "    caches=[]\n",
    "    #Convolution layer 1\n",
    "    Z1, linear_cache = conv_forward(X,W1, b1, hparameters={\"stride\":1,\"pad\":2})\n",
    "    A1, activation_cache = relu(Z1)\n",
    "    A1 = Dropout(A1,keep_prop=0.7)\n",
    "    P1,linear_pool_cache=pool_forward(A1, hparameters={\"stride\":2,\"f\":2}, mode = \"max\")\n",
    "\n",
    "\n",
    "\n",
    "    caches.append((linear_cache,activation_cache,linear_pool_cache))\n",
    "\n",
    "\n",
    "    #Convolution layer 2\n",
    "    Z2,linear_cache=conv_forward(P1,W2, b2, hparameters={\"stride\":1,\"pad\":1})\n",
    "    A2,activation_cache = relu(Z2)\n",
    "    A2 = Dropout(A2,keep_prop=0.60)\n",
    "    P2,linear_pool_cache=pool_forward(A2, hparameters={\"stride\":2,\"f\":2}, mode = \"max\")\n",
    "    #print(P2.shape)\n",
    "\n",
    "    caches.append((linear_cache,activation_cache,linear_pool_cache))\n",
    "\n",
    "\n",
    "    #Convolution layer 3\n",
    "    Z3,linear_cache=conv_forward(P2,W3, b3, hparameters={\"stride\":1,\"pad\":1})\n",
    "    A3,activation_cache = relu(Z3)\n",
    "    A3 = Dropout(A3,keep_prop=0.8)\n",
    "    P3,linear_pool_cache=pool_forward(A3, hparameters={\"stride\":2,\"f\":2}, mode = \"max\")\n",
    "\n",
    "\n",
    "    caches.append((linear_cache,activation_cache,linear_pool_cache))\n",
    "\n",
    "    #Convolution layer 4\n",
    "    Z4,linear_cache=conv_forward(P3,W4, b4, hparameters={\"stride\":1,\"pad\":1})\n",
    "    A4,activation_cache = relu(Z4)\n",
    "    A4 = Dropout(A4,keep_prop=0.6)\n",
    "    P4,linear_pool_cache=pool_forward(A4, hparameters={\"stride\":2,\"f\":2}, mode = \"max\")\n",
    "\n",
    "\n",
    "    caches.append((linear_cache,activation_cache,linear_pool_cache))\n",
    "\n",
    "    #flatten\n",
    "    P4_shape=P4.shape[:]\n",
    "\n",
    "    P4 = torch.t(P4.view(P2.shape[0], -1))\n",
    "\n",
    "    #Fully_Connected\n",
    "\n",
    "    Z5,linear_cache= linear_forward(P4,W5,b5)\n",
    "    A5,activation_cache = relu(Z5)\n",
    "    A5 = Dropout(A5,keep_prop=0.8)\n",
    "    caches.append((linear_cache,activation_cache))\n",
    "\n",
    "    Z6,linear_cache=linear_forward(A5,W6,b6)\n",
    "    A6,activation_cache = softmax(Z6)\n",
    "    caches.append((linear_cache,activation_cache))\n",
    "    cost,AL = cost_function(A6,Y,parameters,lambd = 0.1)\n",
    "    #print(sum(A3[0,:]))\n",
    "    return AL,caches,P4_shape,cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convolution_model(X, Y,mini_batch_size, learning_rate = 0.001, beta = 0.9,beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_iterations= 100, print_cost = True):#lr was 0.009\n",
    "\n",
    "    t = 0\n",
    "    c=0\n",
    "    seed=10\n",
    "    costs = []                     # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "\n",
    "    parameters = initialize_parameters_he() \n",
    "    v,s= initialize_adam(parameters)\n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "        seed+=1\n",
    "        mini_batches=random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "        for j in range(len(mini_batches)):\n",
    "            (mini_batch_x,mini_batch_y)=mini_batches[j]\n",
    "            (mini_batch_x,mini_batch_y) = (Variable(mini_batch_x.cuda()).type(dtype),Variable(mini_batch_y.cuda()).type(dtype))\n",
    "            #mini_batch_y = convert_to_one_hot(mini_batch_y,6) \n",
    " \n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "            AL,caches,P2_shape,cost= forward_prpagation(mini_batch_x,mini_batch_y,parameters)\n",
    "\n",
    "\n",
    "\n",
    "        # Backward propagation.\n",
    "            print(\"backward_beginn\")\n",
    "            grads = Model_Backward(AL,mini_batch_y,caches,P2_shape,parameters,lambd = 0.1)\n",
    "            c+=1\n",
    "            #grads = conv_backward(grads_Z, caches)\n",
    "            print(\"backward_end\")\n",
    "            #difference = gradient_check_n(parameters, grads, mini_batch_x, mini_batch_y)\n",
    "        # Update parameters.\n",
    "            t = t + 1 # Adam counter\n",
    "            parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,t, learning_rate, beta1, beta2,  epsilon)\n",
    "        \n",
    "        # Visualizaing the feature maps\n",
    "\n",
    "            \n",
    " \n",
    "\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 1 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            #predictions_train = predict(train_x_orig, train_yoh, parameters)\n",
    "            costs.append(cost)\n",
    "\n",
    "            \n",
    "            \n",
    "    #plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    \"\"\"\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    fig,axes = plt.subplots(nrows = 2, ncols = 4, figsize=(20,20))\n",
    "    for i in range(2):\n",
    "        for j in range(4):\n",
    "            axes[i,j].imshow(W1[:,:,:,(i*4)+j])\n",
    "    \"\"\"\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backward_beginn\n",
      "backward_end\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n",
      "backward_beginn\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-5ec95dab9513>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConvolution_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x_orig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_yoh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-fe37e6f43f6b>\u001b[0m in \u001b[0;36mConvolution_model\u001b[1;34m(X, Y, mini_batch_size, learning_rate, beta, beta1, beta2, epsilon, num_iterations, print_cost)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# Backward propagation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"backward_beginn\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel_Backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmini_batch_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcaches\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mP2_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlambd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m             \u001b[0mc\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;31m#grads = conv_backward(grads_Z, caches)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-346bbcc97a48>\u001b[0m in \u001b[0;36mModel_Backward\u001b[1;34m(AL, Y, caches, P4_shape, parameters, lambd)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mlinear_cache0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mZ1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpool_cache0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     \u001b[0mdA1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpool_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdP1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool_cache0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"max\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m     \u001b[0mdZ1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrelu_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdA1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mZ1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0mdA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdW1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdb1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconv_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinear_cache0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-4c102d1213aa>\u001b[0m in \u001b[0;36mpool_backward\u001b[1;34m(dA, cache, mode)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                     \u001b[1;31m# Create the mask from a_prev_slice (≈1 line)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m                     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_mask_from_window\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma_prev_slice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma_slice_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                     \u001b[0mdA_prev\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvert_start\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mvert_end\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhoriz_start\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhoriz_end\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-a98d97aad12f>\u001b[0m in \u001b[0;36mcreate_mask_from_window\u001b[1;34m(x, shape)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mArray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[0mat\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mposition\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmax\u001b[0m \u001b[0mentry\u001b[0m \u001b[0mof\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \"\"\"\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parameters = Convolution_model(train_x_orig, train_yoh,mini_batch_size=4, num_iterations = 3, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
